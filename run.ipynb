{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DD KolektorSDD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import (\n",
    "    Activation,\n",
    "    AveragePooling2D,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    MaxPooling2D,\n",
    "    Rescaling,\n",
    "    Input,\n",
    "    BatchNormalization, \n",
    "    Add, \n",
    "    Activation\n",
    ")\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image\n",
    "from PIL import Image, ImageFont\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Disable tenforflow information messages about GPU\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "# Set the random seed\n",
    "\n",
    "random_seed = 42\n",
    "data_dir = \"dataset/KolektorSDD2\"\n",
    "\n",
    "batch_size = 32\n",
    "image_size = (512, 512)\n",
    "input_shape = (512, 512, 3)\n",
    "color_mode = \"rgb\"\n",
    "seed_train_validation = 42\n",
    "shuffle_value = True\n",
    "validation_split = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set memory growth for GPU\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "data_dir = \"dataset/casting_512x512\"\n",
    "\n",
    "batch_size = 32\n",
    "image_size = (512, 512)\n",
    "seed_train_validation = 42\n",
    "shuffle_value = True\n",
    "validation_split = 0.3\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory = data_dir,\n",
    "    labels = 'inferred',\n",
    "    label_mode = 'int',\n",
    "    batch_size = batch_size,\n",
    "    image_size = image_size,\n",
    "    validation_split = validation_split,\n",
    "    subset = \"training\",\n",
    "    seed = seed_train_validation,\n",
    "    color_mode = color_mode,\n",
    "    shuffle = shuffle_value\n",
    "    )\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory = data_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=32,\n",
    "    image_size = image_size,\n",
    "    validation_split = validation_split,\n",
    "    subset = \"validation\",\n",
    "    seed = seed_train_validation,\n",
    "    color_mode = color_mode,\n",
    "    shuffle = shuffle_value\n",
    "    )\n",
    "\n",
    "val_batches = tf.data.experimental.cardinality(val_ds)\n",
    "test_ds = val_ds.take((2*val_batches) // 3)\n",
    "val_ds = val_ds.skip((2*val_batches) // 3)\n",
    "\n",
    "# test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#     directory = test_data_dir,\n",
    "#     labels='inferred',\n",
    "#     label_mode='int',\n",
    "#     image_size = image_size,\n",
    "#     seed = seed_train_validation,\n",
    "#     color_mode = 'rgb',\n",
    "#     shuffle = shuffle_value\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some images\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"), cmap='gray')\n",
    "        plt.title(int(labels[i]))\n",
    "        plt.axis(\"off\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation layer \n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomZoom(0.1),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip(\"vertical\"),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a rescaling layer\n",
    "\n",
    "scaling_layer = Rescaling(1.0 / 255)\n",
    "\n",
    "# Scale the datasets\n",
    "train_data_scaled = train_ds.map(lambda x, y: (scaling_layer(x), y))\n",
    "test_data_scaled = test_ds.map(lambda x, y: (scaling_layer(x), y))\n",
    "val_data_scaled = val_ds.map(lambda x, y: (scaling_layer(x), y))\n",
    "\n",
    "# Test that the scaling has worked by printing the min and max value from one the images\n",
    "image_batch, labels_batch = next(iter(train_data_scaled))\n",
    "\n",
    "\n",
    "image = image_batch[1]\n",
    "print(np.min(image), np.max(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autotune the datasets\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_data_scaled = train_data_scaled.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_data_scaled = test_data_scaled.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_data_scaled = val_data_scaled.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=15,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.1,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0,\n",
    ")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    \"models/model.{epoch:02d}-{val_loss:.2f}.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\",\n",
    ")\n",
    "callback_list = [early_stopping, reduce_lr, model_checkpoint]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple_conv_model\n",
    "\n",
    "simple_conv_model = Sequential([\n",
    "    data_augmentation,\n",
    "\n",
    "    Conv2D(32, 3, padding='same', activation='relu', input_shape=input_shape),\n",
    "    MaxPooling2D(),\n",
    "\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "\n",
    "    Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "\n",
    "    Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the simple_conv_model\n",
    "simple_conv_model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "        \n",
    "# Print the simple_conv_model summary\n",
    "# simple_conv_model.summary()\n",
    "\n",
    "# Train the simple_conv_model\n",
    "epochs = 100\n",
    "history = simple_conv_model.fit(\n",
    "    train_data_scaled,\n",
    "    validation_data=val_data_scaled,\n",
    "    epochs=epochs,\n",
    "    callbacks=callback_list,\n",
    ")\n",
    "# eval and plot simple conv model \n",
    "# Evaluate the simple_conv_model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = simple_conv_model.evaluate(test_data_scaled, batch_size=32)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "\n",
    "# Plot the VGG16 model accuracy and loss curves using `matplotlib`\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train_accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m GlobalAveragePooling2D\n\u001b[1;32m      5\u001b[0m \u001b[39m# Load the VGG16 model without the top classification layer\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m vgg16_base \u001b[39m=\u001b[39m VGG16(weights\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mimagenet\u001b[39;49m\u001b[39m'\u001b[39;49m, include_top\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, input_shape\u001b[39m=\u001b[39;49minput_shape)\n\u001b[1;32m      8\u001b[0m \u001b[39m# Freeze the pre-trained layers in the VGG16 model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m vgg16_base\u001b[39m.\u001b[39mtrainable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/keras/applications/vgg16.py:154\u001b[0m, in \u001b[0;36mVGG16\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    152\u001b[0m         img_input \u001b[39m=\u001b[39m input_tensor\n\u001b[1;32m    153\u001b[0m \u001b[39m# Block 1\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39;49mConv2D(\n\u001b[1;32m    155\u001b[0m     \u001b[39m64\u001b[39;49m, (\u001b[39m3\u001b[39;49m, \u001b[39m3\u001b[39;49m), activation\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m\"\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msame\u001b[39;49m\u001b[39m\"\u001b[39;49m, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mblock1_conv1\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    156\u001b[0m )(img_input)\n\u001b[1;32m    157\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mConv2D(\n\u001b[1;32m    158\u001b[0m     \u001b[39m64\u001b[39m, (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msame\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mblock1_conv2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m )(x)\n\u001b[1;32m    160\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mMaxPooling2D((\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m), strides\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m), name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mblock1_pool\u001b[39m\u001b[39m\"\u001b[39m)(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/keras/engine/base_layer.py:1058\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[39m# Functional Model construction mode is invoked when `Layer`s are called\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[39m# on symbolic `KerasTensor`s, i.e.:\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[39m# >> inputs = tf.keras.Input(10)\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[39m# >> outputs = MyLayer()(inputs)  # Functional construction mode.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[39m# >> model = tf.keras.Model(inputs, outputs)\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[39mif\u001b[39;00m _in_functional_construction_mode(\n\u001b[1;32m   1056\u001b[0m     \u001b[39mself\u001b[39m, inputs, args, kwargs, input_list\n\u001b[1;32m   1057\u001b[0m ):\n\u001b[0;32m-> 1058\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_functional_construction_call(\n\u001b[1;32m   1059\u001b[0m         inputs, args, kwargs, input_list\n\u001b[1;32m   1060\u001b[0m     )\n\u001b[1;32m   1062\u001b[0m \u001b[39m# Maintains info about the `Layer.call` stack.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m call_context \u001b[39m=\u001b[39m base_layer_utils\u001b[39m.\u001b[39mcall_context()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/keras/engine/base_layer.py:2572\u001b[0m, in \u001b[0;36mLayer._functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   2565\u001b[0m         training_arg_passed_by_framework \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   2567\u001b[0m \u001b[39mwith\u001b[39;00m call_context\u001b[39m.\u001b[39menter(\n\u001b[1;32m   2568\u001b[0m     layer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, inputs\u001b[39m=\u001b[39minputs, build_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39mtraining_value\n\u001b[1;32m   2569\u001b[0m ):\n\u001b[1;32m   2570\u001b[0m     \u001b[39m# Check input assumptions set after layer building, e.g. input\u001b[39;00m\n\u001b[1;32m   2571\u001b[0m     \u001b[39m# shape.\u001b[39;00m\n\u001b[0;32m-> 2572\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_keras_tensor_symbolic_call(\n\u001b[1;32m   2573\u001b[0m         inputs, input_masks, args, kwargs\n\u001b[1;32m   2574\u001b[0m     )\n\u001b[1;32m   2576\u001b[0m     \u001b[39mif\u001b[39;00m outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2577\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2578\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA layer\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms `call` method should return a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2579\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTensor or a list of Tensors, not None \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2580\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m(layer: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2581\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/keras/engine/base_layer.py:2419\u001b[0m, in \u001b[0;36mLayer._keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m   2415\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\n\u001b[1;32m   2416\u001b[0m         keras_tensor\u001b[39m.\u001b[39mKerasTensor, output_signature\n\u001b[1;32m   2417\u001b[0m     )\n\u001b[1;32m   2418\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2419\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer_output_signature(\n\u001b[1;32m   2420\u001b[0m         inputs, args, kwargs, input_masks\n\u001b[1;32m   2421\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/keras/engine/base_layer.py:2476\u001b[0m, in \u001b[0;36mLayer._infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m   2468\u001b[0m \u001b[39mwith\u001b[39;00m backend\u001b[39m.\u001b[39mname_scope(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name_scope()):\n\u001b[1;32m   2469\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   2470\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   2471\u001b[0m     ):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2474\u001b[0m         \u001b[39m# TODO(kaftan): do we maybe_build here, or have we already\u001b[39;00m\n\u001b[1;32m   2475\u001b[0m         \u001b[39m# done it?\u001b[39;00m\n\u001b[0;32m-> 2476\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_build(inputs)\n\u001b[1;32m   2477\u001b[0m         inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs)\n\u001b[1;32m   2478\u001b[0m         outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/keras/engine/base_layer.py:3023\u001b[0m, in \u001b[0;36mLayer._maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3018\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild, \u001b[39m\"\u001b[39m\u001b[39m_is_default\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   3019\u001b[0m     \u001b[39m# Any setup work performed only once should happen in an\u001b[39;00m\n\u001b[1;32m   3020\u001b[0m     \u001b[39m# `init_scope` to avoid creating symbolic Tensors that will\u001b[39;00m\n\u001b[1;32m   3021\u001b[0m     \u001b[39m# later pollute any eager operations.\u001b[39;00m\n\u001b[1;32m   3022\u001b[0m     \u001b[39mwith\u001b[39;00m tf_utils\u001b[39m.\u001b[39mmaybe_init_scope(\u001b[39mself\u001b[39m):\n\u001b[0;32m-> 3023\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild(input_shapes)\n\u001b[1;32m   3024\u001b[0m \u001b[39m# We must set also ensure that the layer is marked as built, and the\u001b[39;00m\n\u001b[1;32m   3025\u001b[0m \u001b[39m# build shape is stored since user defined build functions may not\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m \u001b[39m# be calling `super.build()`\u001b[39;00m\n\u001b[1;32m   3027\u001b[0m Layer\u001b[39m.\u001b[39mbuild(\u001b[39mself\u001b[39m, input_shapes)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/keras/layers/convolutional/base_conv.py:227\u001b[0m, in \u001b[0;36mConv.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39m# compute_output_shape contains some validation logic for the input\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39m# shape, and make sure the output shape has all positive dimensions.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_output_shape(input_shape)\n\u001b[0;32m--> 227\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_weight(\n\u001b[1;32m    228\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mkernel\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    229\u001b[0m     shape\u001b[39m=\u001b[39;49mkernel_shape,\n\u001b[1;32m    230\u001b[0m     initializer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_initializer,\n\u001b[1;32m    231\u001b[0m     regularizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_regularizer,\n\u001b[1;32m    232\u001b[0m     constraint\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_constraint,\n\u001b[1;32m    233\u001b[0m     trainable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    234\u001b[0m     dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype,\n\u001b[1;32m    235\u001b[0m )\n\u001b[1;32m    236\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_bias:\n\u001b[1;32m    237\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_weight(\n\u001b[1;32m    238\u001b[0m         name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    239\u001b[0m         shape\u001b[39m=\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilters,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m         dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype,\n\u001b[1;32m    245\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/keras/engine/base_layer.py:712\u001b[0m, in \u001b[0;36mLayer.add_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[39mif\u001b[39;00m layout:\n\u001b[1;32m    710\u001b[0m     getter \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(getter, layout\u001b[39m=\u001b[39mlayout)\n\u001b[0;32m--> 712\u001b[0m variable \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_variable_with_custom_getter(\n\u001b[1;32m    713\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    714\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m    715\u001b[0m     \u001b[39m# TODO(allenl): a `make_variable` equivalent should be added as a\u001b[39;49;00m\n\u001b[1;32m    716\u001b[0m     \u001b[39m# `Trackable` method.\u001b[39;49;00m\n\u001b[1;32m    717\u001b[0m     getter\u001b[39m=\u001b[39;49mgetter,\n\u001b[1;32m    718\u001b[0m     \u001b[39m# Manage errors in Layer rather than Trackable.\u001b[39;49;00m\n\u001b[1;32m    719\u001b[0m     overwrite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    720\u001b[0m     initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[1;32m    721\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    722\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[1;32m    723\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m    724\u001b[0m     use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[1;32m    725\u001b[0m     collections\u001b[39m=\u001b[39;49mcollections_arg,\n\u001b[1;32m    726\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[1;32m    727\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[1;32m    728\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[1;32m    729\u001b[0m )\n\u001b[1;32m    730\u001b[0m \u001b[39mif\u001b[39;00m regularizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[39m# TODO(fchollet): in the future, this should be handled at the\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[39m# level of variable creation, and weight regularization losses\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[39m# should be variable attributes.\u001b[39;00m\n\u001b[1;32m    734\u001b[0m     name_in_scope \u001b[39m=\u001b[39m variable\u001b[39m.\u001b[39mname[: variable\u001b[39m.\u001b[39mname\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)]\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/trackable/base.py:489\u001b[0m, in \u001b[0;36mTrackable._add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    479\u001b[0m   \u001b[39mif\u001b[39;00m (checkpoint_initializer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    480\u001b[0m       \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(initializer, CheckpointInitialValueCallable) \u001b[39mand\u001b[39;00m\n\u001b[1;32m    481\u001b[0m            (initializer\u001b[39m.\u001b[39mrestore_uid \u001b[39m>\u001b[39m checkpoint_initializer\u001b[39m.\u001b[39mrestore_uid))):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[39m# then we'll catch that when we call _track_trackable. So this is\u001b[39;00m\n\u001b[1;32m    487\u001b[0m     \u001b[39m# \"best effort\" to set the initializer with the highest restore UID.\u001b[39;00m\n\u001b[1;32m    488\u001b[0m     initializer \u001b[39m=\u001b[39m checkpoint_initializer\n\u001b[0;32m--> 489\u001b[0m new_variable \u001b[39m=\u001b[39m getter(\n\u001b[1;32m    490\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    491\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m    492\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    493\u001b[0m     initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[1;32m    494\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_for_getter)\n\u001b[1;32m    496\u001b[0m \u001b[39m# If we set an initializer and the variable processed it, tracking will not\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[39m# assign again. It will add this variable to our dependencies, and if there\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[39m# is a non-trivial restoration queued, it will handle that. This also\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[39m# handles slot variables.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m overwrite \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(new_variable, Trackable):\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/keras/engine/base_layer_utils.py:134\u001b[0m, in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner, layout)\u001b[0m\n\u001b[1;32m    127\u001b[0m     use_resource \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m layout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[39m# In theory, in `use_resource` is True and `collections` is empty\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[39m# (that is to say, in TF2), we can use tf.Variable.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[39m# However, this breaks legacy (Estimator) checkpoints because\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[39m# it changes variable names. Remove this when V1 is fully deprecated.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m tf1\u001b[39m.\u001b[39;49mVariable(\n\u001b[1;32m    135\u001b[0m         initial_value\u001b[39m=\u001b[39;49minit_val,\n\u001b[1;32m    136\u001b[0m         name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    137\u001b[0m         trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m    138\u001b[0m         caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[1;32m    139\u001b[0m         dtype\u001b[39m=\u001b[39;49mvariable_dtype,\n\u001b[1;32m    140\u001b[0m         validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[1;32m    141\u001b[0m         constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[1;32m    142\u001b[0m         use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[1;32m    143\u001b[0m         collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[1;32m    144\u001b[0m         synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[1;32m    145\u001b[0m         aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[1;32m    146\u001b[0m         shape\u001b[39m=\u001b[39;49mvariable_shape \u001b[39mif\u001b[39;49;00m variable_shape \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    147\u001b[0m     )\n\u001b[1;32m    148\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[39mreturn\u001b[39;00m dtensor\u001b[39m.\u001b[39mDVariable(\n\u001b[1;32m    150\u001b[0m         initial_value\u001b[39m=\u001b[39minit_val,\n\u001b[1;32m    151\u001b[0m         name\u001b[39m=\u001b[39mname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         shape\u001b[39m=\u001b[39mvariable_shape \u001b[39mif\u001b[39;00m variable_shape \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    161\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/ops/variables.py:285\u001b[0m, in \u001b[0;36mVariableMetaclass.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[39m@traceback_utils\u001b[39m\u001b[39m.\u001b[39mfilter_traceback\n\u001b[1;32m    283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    284\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m VariableV1:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_v1_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    286\u001b[0m   \u001b[39melif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m Variable:\n\u001b[1;32m    287\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_variable_v2_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/ops/variables.py:226\u001b[0m, in \u001b[0;36mVariableMetaclass._variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39mif\u001b[39;00m aggregation \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m   aggregation \u001b[39m=\u001b[39m VariableAggregation\u001b[39m.\u001b[39mNONE\n\u001b[0;32m--> 226\u001b[0m \u001b[39mreturn\u001b[39;00m previous_getter(\n\u001b[1;32m    227\u001b[0m     initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[1;32m    228\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m    229\u001b[0m     collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[1;32m    230\u001b[0m     validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[1;32m    231\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[1;32m    232\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    233\u001b[0m     variable_def\u001b[39m=\u001b[39;49mvariable_def,\n\u001b[1;32m    234\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    235\u001b[0m     expected_shape\u001b[39m=\u001b[39;49mexpected_shape,\n\u001b[1;32m    236\u001b[0m     import_scope\u001b[39m=\u001b[39;49mimport_scope,\n\u001b[1;32m    237\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[1;32m    238\u001b[0m     use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[1;32m    239\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[1;32m    240\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[1;32m    241\u001b[0m     shape\u001b[39m=\u001b[39;49mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/ops/variables.py:219\u001b[0m, in \u001b[0;36mVariableMetaclass._variable_v1_call.<locals>.<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_variable_v1_call\u001b[39m(\u001b[39mcls\u001b[39m,\n\u001b[1;32m    203\u001b[0m                       initial_value\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    204\u001b[0m                       trainable\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m                       aggregation\u001b[39m=\u001b[39mVariableAggregation\u001b[39m.\u001b[39mNONE,\n\u001b[1;32m    217\u001b[0m                       shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    218\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m   previous_getter \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: default_variable_creator(\u001b[39mNone\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    220\u001b[0m   \u001b[39mfor\u001b[39;00m _, getter \u001b[39min\u001b[39;00m ops\u001b[39m.\u001b[39mget_default_graph()\u001b[39m.\u001b[39m_variable_creator_stack:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     previous_getter \u001b[39m=\u001b[39m _make_getter(getter, previous_getter)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/ops/variable_scope.py:2707\u001b[0m, in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2705\u001b[0m \u001b[39mif\u001b[39;00m use_resource:\n\u001b[1;32m   2706\u001b[0m   distribute_strategy \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdistribute_strategy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 2707\u001b[0m   \u001b[39mreturn\u001b[39;00m resource_variable_ops\u001b[39m.\u001b[39;49mResourceVariable(\n\u001b[1;32m   2708\u001b[0m       initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[1;32m   2709\u001b[0m       trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m   2710\u001b[0m       collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[1;32m   2711\u001b[0m       validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[1;32m   2712\u001b[0m       caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[1;32m   2713\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   2714\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   2715\u001b[0m       constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[1;32m   2716\u001b[0m       variable_def\u001b[39m=\u001b[39;49mvariable_def,\n\u001b[1;32m   2717\u001b[0m       import_scope\u001b[39m=\u001b[39;49mimport_scope,\n\u001b[1;32m   2718\u001b[0m       distribute_strategy\u001b[39m=\u001b[39;49mdistribute_strategy,\n\u001b[1;32m   2719\u001b[0m       synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[1;32m   2720\u001b[0m       aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[1;32m   2721\u001b[0m       shape\u001b[39m=\u001b[39;49mshape)\n\u001b[1;32m   2722\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2723\u001b[0m   \u001b[39mreturn\u001b[39;00m variables\u001b[39m.\u001b[39mRefVariable(\n\u001b[1;32m   2724\u001b[0m       initial_value\u001b[39m=\u001b[39minitial_value,\n\u001b[1;32m   2725\u001b[0m       trainable\u001b[39m=\u001b[39mtrainable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2736\u001b[0m       aggregation\u001b[39m=\u001b[39maggregation,\n\u001b[1;32m   2737\u001b[0m       shape\u001b[39m=\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/ops/variables.py:289\u001b[0m, in \u001b[0;36mVariableMetaclass.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_variable_v2_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    288\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 289\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(VariableMetaclass, \u001b[39mcls\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py:1768\u001b[0m, in \u001b[0;36mResourceVariable.__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape, handle, experimental_enable_variable_lifting)\u001b[0m\n\u001b[1;32m   1763\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_from_handle(trainable\u001b[39m=\u001b[39mtrainable,\n\u001b[1;32m   1764\u001b[0m                          shape\u001b[39m=\u001b[39mshape,\n\u001b[1;32m   1765\u001b[0m                          dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1766\u001b[0m                          handle\u001b[39m=\u001b[39mhandle)\n\u001b[1;32m   1767\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1768\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_from_args(\n\u001b[1;32m   1769\u001b[0m       initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[1;32m   1770\u001b[0m       trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m   1771\u001b[0m       collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[1;32m   1772\u001b[0m       caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[1;32m   1773\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1774\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1775\u001b[0m       constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[1;32m   1776\u001b[0m       synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[1;32m   1777\u001b[0m       aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[1;32m   1778\u001b[0m       shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m   1779\u001b[0m       distribute_strategy\u001b[39m=\u001b[39;49mdistribute_strategy,\n\u001b[1;32m   1780\u001b[0m       validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[1;32m   1781\u001b[0m       experimental_enable_variable_lifting\u001b[39m=\u001b[39;49mexperimental_enable_variable_lifting,\n\u001b[1;32m   1782\u001b[0m       )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py:1952\u001b[0m, in \u001b[0;36mResourceVariable._init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape, validate_shape, experimental_enable_variable_lifting)\u001b[0m\n\u001b[1;32m   1950\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(\u001b[39m\"\u001b[39m\u001b[39mInitializer\u001b[39m\u001b[39m\"\u001b[39m), device_context_manager(\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1951\u001b[0m   \u001b[39mif\u001b[39;00m init_from_fn:\n\u001b[0;32m-> 1952\u001b[0m     initial_value \u001b[39m=\u001b[39m initial_value()\n\u001b[1;32m   1953\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(initial_value, trackable\u001b[39m.\u001b[39mCheckpointInitialValue):\n\u001b[1;32m   1954\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_initialize_trackable()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/keras/initializers/initializers.py:637\u001b[0m, in \u001b[0;36mVarianceScaling.__call__\u001b[0;34m(self, shape, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m     _ensure_keras_seeded()\n\u001b[1;32m    630\u001b[0m     \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39mcall_with_layout(\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_init_val,\n\u001b[1;32m    632\u001b[0m         layout,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    635\u001b[0m         nonce\u001b[39m=\u001b[39mnonce,\n\u001b[1;32m    636\u001b[0m     )\n\u001b[0;32m--> 637\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_init_val(shape\u001b[39m=\u001b[39;49mshape, dtype\u001b[39m=\u001b[39;49mdtype, nonce\u001b[39m=\u001b[39;49mnonce)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/keras/initializers/initializers.py:662\u001b[0m, in \u001b[0;36mVarianceScaling._generate_init_val\u001b[0;34m(self, shape, dtype, nonce)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m     limit \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m \u001b[39m*\u001b[39m scale)\n\u001b[0;32m--> 662\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_random_generator\u001b[39m.\u001b[39;49mrandom_uniform(\n\u001b[1;32m    663\u001b[0m         shape, \u001b[39m-\u001b[39;49mlimit, limit, dtype, nonce\n\u001b[1;32m    664\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/keras/backend.py:2101\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[0;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[1;32m   2099\u001b[0m     \u001b[39mif\u001b[39;00m nonce:\n\u001b[1;32m   2100\u001b[0m         seed \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[0;32m-> 2101\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mstateless_uniform(\n\u001b[1;32m   2102\u001b[0m         shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m   2103\u001b[0m         minval\u001b[39m=\u001b[39;49mminval,\n\u001b[1;32m   2104\u001b[0m         maxval\u001b[39m=\u001b[39;49mmaxval,\n\u001b[1;32m   2105\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   2106\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m   2107\u001b[0m     )\n\u001b[1;32m   2108\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(\n\u001b[1;32m   2109\u001b[0m     shape\u001b[39m=\u001b[39mshape,\n\u001b[1;32m   2110\u001b[0m     minval\u001b[39m=\u001b[39mminval,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2113\u001b[0m     seed\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_legacy_seed(),\n\u001b[1;32m   2114\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/ops/stateless_random_ops.py:500\u001b[0m, in \u001b[0;36mstateless_random_uniform\u001b[0;34m(shape, seed, minval, maxval, dtype, name, alg)\u001b[0m\n\u001b[1;32m    497\u001b[0m   maxval \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    498\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(name, \u001b[39m\"\u001b[39m\u001b[39mstateless_random_uniform\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    499\u001b[0m                     [shape, seed, minval, maxval]) \u001b[39mas\u001b[39;00m name:\n\u001b[0;32m--> 500\u001b[0m   shape \u001b[39m=\u001b[39m tensor_util\u001b[39m.\u001b[39;49mshape_tensor(shape)\n\u001b[1;32m    501\u001b[0m   \u001b[39mif\u001b[39;00m dtype\u001b[39m.\u001b[39mis_integer \u001b[39mand\u001b[39;00m minval \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     key, counter, alg \u001b[39m=\u001b[39m _get_key_counter_alg(seed, alg)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/framework/tensor_util.py:1150\u001b[0m, in \u001b[0;36mshape_tensor\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m   1144\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1145\u001b[0m     \u001b[39m# If there are Dimension objects in the shape, unwrap them. This can be a\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m     \u001b[39m# problem if v1 and v2 TensorShape objects get mixed up in partial\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m     \u001b[39m# conversions, leading to shapes such as (1, 2, Dimension(5)), which are\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m     \u001b[39m# not convertible to Tensors because of mixed content.\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m     shape \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mmap\u001b[39m(tensor_shape\u001b[39m.\u001b[39mdimension_value, shape))\n\u001b[0;32m-> 1150\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(shape, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mshape\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1642\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1633\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1634\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1635\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1638\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1639\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[1;32m   1641\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1642\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1644\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1645\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:344\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    342\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    343\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[0;32m--> 344\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:268\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    173\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \n\u001b[1;32m    175\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    269\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:280\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    279\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 280\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    282\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[1;32m    283\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:305\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    304\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    306\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# keras vgg16 model\n",
    "from keras.applications import VGG16\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "# Load the VGG16 model without the top classification layer\n",
    "vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "# Freeze the pre-trained layers in the VGG16 model\n",
    "vgg16_base.trainable = False\n",
    "\n",
    "# Create a sequential model\n",
    "vgg16_model = Sequential([\n",
    "    data_augmentation,\n",
    "    vgg16_base,  # Add the VGG16 base model\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the VGG16 model\n",
    "vgg16_model.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# Train the VGG16 model\n",
    "epochs = 100\n",
    "history = vgg16_model.fit(\n",
    "    train_data_scaled,\n",
    "    validation_data=val_data_scaled,\n",
    "    epochs=epochs,\n",
    "    callbacks=callback_list\n",
    ")\n",
    "# plot the vgg16 model performance \n",
    "# Evaluate the VGG16 model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = vgg16_model.evaluate(test_data_scaled, batch_size=32)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Plot the VGG16 model accuracy and loss curves using `matplotlib`\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train_accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras Inception model\n",
    "from keras.applications import InceptionV3\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "# Load the InceptionV3 model without the top classification layer\n",
    "inception_base = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "# Freeze the pre-trained layers in the InceptionV3 model\n",
    "inception_base.trainable = False\n",
    "\n",
    "# Create a sequential model\n",
    "inception_model = Sequential([\n",
    "    data_augmentation,\n",
    "    inception_base,  # Add the InceptionV3 base model\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the InceptionV3 model\n",
    "inception_model.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# Train the InceptionV3 model\n",
    "epochs = 100\n",
    "history = inception_model.fit(\n",
    "    train_data_scaled,\n",
    "    validation_data=val_data_scaled,\n",
    "    epochs=epochs,\n",
    "    callbacks=callback_list\n",
    ")\n",
    "\n",
    "# plot the inception model performance\n",
    "# Evaluate the InceptionV3 model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = inception_model.evaluate(test_data_scaled, batch_size=32)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Plot the InceptionV3 model accuracy and loss curves using `matplotlib` fixed 0 to 1 scale\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train_accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras mobilenet model\n",
    "from keras.applications import MobileNet\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "# Load the MobileNet model without the top classification layer\n",
    "mobilenet_base = MobileNet(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "# Freeze the pre-trained layers in the MobileNet model\n",
    "mobilenet_base.trainable = False\n",
    "\n",
    "# Create a sequential model\n",
    "mobilenet_model = Sequential([\n",
    "    data_augmentation,\n",
    "    mobilenet_base,  # Add the MobileNet base model\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the MobileNet model\n",
    "mobilenet_model.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# Train the MobileNet model\n",
    "epochs = 100\n",
    "history = mobilenet_model.fit(\n",
    "    train_data_scaled,\n",
    "    validation_data=val_data_scaled,\n",
    "    epochs=epochs,\n",
    "    callbacks=callback_list\n",
    ")\n",
    "\n",
    "# plot the mobilenet model performance\n",
    "# Evaluate the MobileNet model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = mobilenet_model.evaluate(test_data_scaled, batch_size=32)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Plot the MobileNet model accuracy and loss curves using `matplotlib` fixed 0 to 1 scale\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train_accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras resnet model\n",
    "from keras.applications import ResNet50\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "# Load the ResNet50 model without the top classification layer\n",
    "resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "# Freeze the pre-trained layers in the ResNet50 model\n",
    "resnet_base.trainable = False\n",
    "\n",
    "# Create a sequential model\n",
    "resnet_model = Sequential([\n",
    "    data_augmentation,\n",
    "    resnet_base,  # Add the ResNet50 base model\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the ResNet50 model\n",
    "resnet_model.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# Train the ResNet50 model\n",
    "epochs = 100\n",
    "history = resnet_model.fit(\n",
    "    train_data_scaled,\n",
    "    validation_data=val_data_scaled,\n",
    "    epochs=epochs,\n",
    "    callbacks=callback_list\n",
    ")\n",
    "\n",
    "# plot the resnet model performance\n",
    "# Evaluate the ResNet50 model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = resnet_model.evaluate(test_data_scaled, batch_size=32)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Plot the ResNet50 model accuracy and loss curves using `matplotlib` fixed 0 to 1 scale\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train_accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
